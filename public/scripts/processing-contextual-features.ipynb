{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python368jvsc74a57bd06ed596c67471c5aea1ce66a7249b4f065011d8eb769856b555abbd0d9e4d28b9",
   "display_name": "Python 3.6.8 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "systemsDict = {\n",
    "    'FIELD': {\n",
    "        'ABSTRACTNESS-TYPE': [\n",
    "            'conceptual-ie-internally-oriented',\n",
    "            'practical-ie-outwardly-oriented'\n",
    "        ],\n",
    "        'ACTIVITY-FOCUS': [\n",
    "            'action',\n",
    "            'relation',\n",
    "            'contact',\n",
    "        ],\n",
    "        'GOALS-TYPE': [\n",
    "            'instructing',\n",
    "            'projecting',\n",
    "            'asserting',\n",
    "        ],\n",
    "    },\n",
    "    'TENOR': {\n",
    "        'VALUE-ORIENTATION-PREDISPOSITION': [\n",
    "            'allying',\n",
    "            'opposing',\n",
    "        ],\n",
    "\n",
    "        'PUBLICITY': [\n",
    "            'disinterested',\n",
    "            'neutral',\n",
    "            'on-someones-side',\n",
    "            'private',\n",
    "        ],\n",
    "        'NUMBER-OF-SPEAKING-PARTICIPANTS': [\n",
    "            'monological',\n",
    "            'dialogical',\n",
    "            'multilogical',\n",
    "        ],\n",
    "        'CONTROL-TYPE': [\n",
    "            'institutional',\n",
    "            'non-institutional-or-neutralized',\n",
    "            'unclear',\n",
    "            'equalized',\n",
    "        ],\n",
    "        'SOCIAL-DISTANCE': [\n",
    "            'close',\n",
    "            'distant',\n",
    "        ],\n",
    "    },\n",
    "    'MODE': {\n",
    "        'LANGUAGE-ROLE-TYPE': [\n",
    "            'constitutive',\n",
    "            'ancillary',\n",
    "        ],\n",
    "        'PROCESS-SHARING-TYPE': [\n",
    "            'addressee-more-active',\n",
    "            'addressee-more-passive',\n",
    "        ],\n",
    "        'CHANNEL-TYPE': [\n",
    "            'phonic',\n",
    "            'graphic',\n",
    "        ],\n",
    "        'MEDIUM-TYPE': [\n",
    "            'spoken',\n",
    "            'written',\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "allSituationalChoices = [\n",
    "  'no embedded discourse',\n",
    "  'excluded discourse only',\n",
    "  'practical-ie-outwardly-oriented',\n",
    "  'action',\n",
    "  'projecting',\n",
    "  'allying',\n",
    "  'private',\n",
    "  'monological',\n",
    "  'non-institutional-or-neutralized',\n",
    "  'distant',\n",
    "  'ancillary',\n",
    "  'addressee-more-passive',\n",
    "  'spoken',\n",
    "  'phonic',\n",
    "  'multilogical',\n",
    "  'institutional',\n",
    "  'addressee-more-active',\n",
    "  'conceptual-ie-internally-oriented',\n",
    "  'relation',\n",
    "  'instructing',\n",
    "  'opposing',\n",
    "  'neutral',\n",
    "  'unclear',\n",
    "  'asserting',\n",
    "  'close',\n",
    "  'dialogical',\n",
    "  'constitutive',\n",
    "  'disinterested',\n",
    "  'written',\n",
    "  'on-someones-side',\n",
    "  'graphic',\n",
    "  'contact',\n",
    "  'equalized',\n",
    "]\n",
    "\n",
    "def getRegisterParameterFromFeature(feature):\n",
    "    for fieldSystem in systemsDict['FIELD'].keys():\n",
    "        features = systemsDict['FIELD'][fieldSystem]\n",
    "        if feature in features:\n",
    "            return 'FIELD'\n",
    "    for tenorSystem in systemsDict['TENOR'].keys():\n",
    "        features = systemsDict['TENOR'][tenorSystem]\n",
    "        if feature in features:\n",
    "            return 'TENOR'\n",
    "    for modeSystem in systemsDict['MODE'].keys():\n",
    "        features = systemsDict['MODE'][modeSystem]\n",
    "        if feature in features:\n",
    "            return 'MODE'\n",
    "\n",
    "def getSystemFromFeature(feature):\n",
    "    for fieldSystem in systemsDict['FIELD'].keys():\n",
    "        features = systemsDict['FIELD'][fieldSystem]\n",
    "        if feature in features:\n",
    "            return (fieldSystem, feature)\n",
    "    for tenorSystem in systemsDict['TENOR'].keys():\n",
    "        features = systemsDict['TENOR'][tenorSystem]\n",
    "        if feature in features:\n",
    "            return (tenorSystem, feature)\n",
    "    for modeSystem in systemsDict['MODE'].keys():\n",
    "        features = systemsDict['MODE'][modeSystem]\n",
    "        if feature in features:\n",
    "            return (modeSystem, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS BLOCK IF YOU WANT TO SAVE THE OUTPUT WITH FEATURES ASSIGNED TO SYSTEMS\n",
    "\n",
    "\n",
    "# inputFilePath = '/Volumes/Storage/Programming/dissertation-research/csv-data/dynamic-features-gospels.tsv'\n",
    "\n",
    "# with open(inputFilePath, encoding='utf8') as inputFile:\n",
    "#     with open('dynamic-features-gospels_sorted.tsv', 'a', encoding='utf8') as outputFile:\n",
    "#         for line in inputFile:\n",
    "#             cells = line.split('\\t')\n",
    "#             id = cells[0]\n",
    "#             title = cells[1]\n",
    "#             pre = cells[2]\n",
    "#             via = cells[3]\n",
    "#             try:\n",
    "#                 preList = eval(pre)\n",
    "#                 viaList = eval(via)\n",
    "#                 preTextFeatures = [getSystemFromFeature(feature) for feature in preList]\n",
    "#                 viaTextFeatures = [getSystemFromFeature(feature) for feature in viaList]\n",
    "#                 outputFile.write(f'{id}\\t{title}\\t{preTextFeatures}\\t{viaTextFeatures}\\n')\n",
    "#             except Exception as error: # in case the feature list is only a string \n",
    "#                 # preTextFeatures = sorted(cells[2])\n",
    "#                 # viaTextFeatures = sorted(cells[3])\n",
    "#                 # (e.g. 'no embedded discourse' or column header)\n",
    "#                 print(error)\n",
    "#                 print('Previous line error location: ' + line)\n",
    "#                 outputFile.write(f'{id}\\t{title}\\t{pre}\\t{via}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(len(set(allChanges)))\n",
    "# for mutation in set(allChanges):\n",
    "#     print(f'{mutation[0]} -> {mutation[1]}')\n",
    "\n",
    "# print(set(allChanges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "inputFilePath = '/Volumes/Storage/Programming/dissertation-research/csv-data/dynamic-features-gospels.tsv'\n",
    "\n",
    "with open(inputFilePath, encoding='utf8') as inputFile:\n",
    "    with open('dynamic-features-only-gospels.tsv', 'a', encoding='utf8') as outputFile:\n",
    "        allChanges = list()\n",
    "        for line in inputFile:\n",
    "            cells = line.split('\\t')\n",
    "            id = cells[0]\n",
    "            title = cells[1]\n",
    "            print(f'{id} {title}')\n",
    "            outputFile.write(f'{id} {title}\\n')\n",
    "            pre = cells[2]\n",
    "            via = cells[3]\n",
    "            try:\n",
    "                preList = eval(pre)\n",
    "                viaList = eval(via)\n",
    "                preTextFeatures = [getSystemFromFeature(feature) for feature in preList]\n",
    "                viaTextFeatures = [getSystemFromFeature(feature) for feature in viaList]\n",
    "                preTextFeatures_sorted = sorted(preTextFeatures, key=lambda tupple: tupple[0])\n",
    "                viaTextFeatures_sorted = sorted(viaTextFeatures, key=lambda tupple: tupple[0])\n",
    "                \n",
    "                preTextFeatures_values_only = [featureAndSystem[1] for featureAndSystem in preTextFeatures_sorted]\n",
    "                viaTextFeatures_values_only = [featureAndSystem[1] for featureAndSystem in viaTextFeatures_sorted]\n",
    "                \n",
    "                zipped = zip(preTextFeatures_values_only, viaTextFeatures_values_only)\n",
    "                counter = 0\n",
    "                for pair in zipped:\n",
    "                    registerParameter = getRegisterParameterFromFeature(pair[0])\n",
    "                    if pair[0].strip() != pair[1].strip():\n",
    "                        counter += 1\n",
    "                        allChanges.append(pair)\n",
    "                        print(registerParameter, f'{pair}')\n",
    "                        outputFile.write(f'{registerParameter}, {pair[0]} -> {pair[1]}\\n')\n",
    "                    else:\n",
    "                        pass\n",
    "                # print(f'{counter}\\n\\n')\n",
    "                outputFile.write(f'{counter}\\n\\n')\n",
    "                print()\n",
    "            except Exception as error: \n",
    "                # in case the feature list is only a string \n",
    "                # (e.g. 'no embedded discourse' or column header)\n",
    "                # print(error)\n",
    "                # print('Previous line error location: ' + line)\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RECORD ONLY DIFFERENCE BETWEEN PRE AND VIA TEXT FEATURES\n",
    "inputFilePath = '/Volumes/Storage/Programming/dissertation-research/csv-data/dynamic-features-gospels.tsv'\n",
    "with open(inputFilePath, encoding='utf8') as inputFile:\n",
    "    with open('dynamic-features-vectors-gospels.tsv', 'a', encoding='utf8') as outputFile:\n",
    "        outputFile.write(\"SITUATION\")\n",
    "        for mutation in allMutations:\n",
    "            outputFile.write(';{mutation}'.format(mutation=mutation)) # NOTE: semicolon separator because mutations and titles are CSV\n",
    "        outputFile.write('\\n')\n",
    "\n",
    "        for situationRow in inputFile:\n",
    "            outputRow = [] \n",
    "            cells = situationRow.split('\\t')\n",
    "            id = cells[0]\n",
    "            title = cells[1]\n",
    "            outputRow.append(f'{id} {title}')\n",
    "            pre = cells[2]\n",
    "            via = cells[3]\n",
    "            try:\n",
    "                preList = eval(pre)\n",
    "                viaList = eval(via)\n",
    "                preTextFeatures = [getSystemFromFeature(feature) for feature in preList]\n",
    "                viaTextFeatures = [getSystemFromFeature(feature) for feature in viaList]\n",
    "                preTextFeatures_sorted = sorted(preTextFeatures, key=lambda tupple: tupple[0])\n",
    "                viaTextFeatures_sorted = sorted(viaTextFeatures, key=lambda tupple: tupple[0])\n",
    "                \n",
    "                preTextFeatures_values_only = [featureAndSystem[1] for featureAndSystem in preTextFeatures_sorted]\n",
    "                viaTextFeatures_values_only = [featureAndSystem[1] for featureAndSystem in viaTextFeatures_sorted]\n",
    "                \n",
    "                zippedFeatures = zip(preTextFeatures_values_only, viaTextFeatures_values_only)\n",
    "                zippedFeaturesList = zippedFeatures\n",
    "                mutatedFeatures = []\n",
    "                for featurePair in zippedFeaturesList:\n",
    "                    if featurePair[0] == featurePair[1]:\n",
    "                        pass\n",
    "                    else:\n",
    "                        mutatedFeatures.append(featurePair)\n",
    "\n",
    "                for mutation in allMutations:\n",
    "                    if mutation in set(mutatedFeatures):\n",
    "                        outputRow.append('1')\n",
    "                    else:\n",
    "                        outputRow.append('0')\n",
    "\n",
    "                outputFile.write(';'.join(outputRow)) # NOTE: semicolon separator because mutations and titles are CSV\n",
    "                outputFile.write('\\n')\n",
    "            except Exception as error: \n",
    "                # in case the feature list is only a string \n",
    "                # (e.g. 'no embedded discourse' or column header)\n",
    "                # print(error)\n",
    "                # print('Previous line error location: ' + line)\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allMutations = set(allChanges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "31\n31\n62\n['pre-practical-ie-outwardly-oriented', 'pre-action', 'pre-projecting', 'pre-allying', 'pre-private', 'pre-monological', 'pre-non-institutional-or-neutralized', 'pre-distant', 'pre-ancillary', 'pre-addressee-more-passive', 'pre-spoken', 'pre-phonic', 'pre-multilogical', 'pre-institutional', 'pre-addressee-more-active', 'pre-conceptual-ie-internally-oriented', 'pre-relation', 'pre-instructing', 'pre-opposing', 'pre-neutral', 'pre-unclear', 'pre-asserting', 'pre-close', 'pre-dialogical', 'pre-constitutive', 'pre-disinterested', 'pre-written', 'pre-on-someones-side', 'pre-graphic', 'pre-contact', 'pre-equalized', 'via-practical-ie-outwardly-oriented', 'via-action', 'via-projecting', 'via-allying', 'via-private', 'via-monological', 'via-non-institutional-or-neutralized', 'via-distant', 'via-ancillary', 'via-addressee-more-passive', 'via-spoken', 'via-phonic', 'via-multilogical', 'via-institutional', 'via-addressee-more-active', 'via-conceptual-ie-internally-oriented', 'via-relation', 'via-instructing', 'via-opposing', 'via-neutral', 'via-unclear', 'via-asserting', 'via-close', 'via-dialogical', 'via-constitutive', 'via-disinterested', 'via-written', 'via-on-someones-side', 'via-graphic', 'via-contact', 'via-equalized']\n"
     ]
    }
   ],
   "source": [
    "preTextFeatureHeaders = [f'pre-{feature}' for feature in allSituationalChoices if feature != 'no embedded discourse' and feature != 'excluded discourse only']\n",
    "\n",
    "viaTextFeatureHeaders = [f'via-{feature}' for feature in allSituationalChoices if feature != 'no embedded discourse' and feature != 'excluded discourse only']\n",
    "\n",
    "combinedFeatureHeaders = [*preTextFeatureHeaders, *viaTextFeatureHeaders]\n",
    "\n",
    "print(len(preTextFeatureHeaders))\n",
    "print(len(viaTextFeatureHeaders))\n",
    "print(len(combinedFeatureHeaders))\n",
    "print(combinedFeatureHeaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'allSituationalChoicesWithFeatures' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-eefdb25eeae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PRE-VIA-vectors-gospels.tsv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutputFile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutputFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SITUATION\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallSituationalChoicesWithFeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0moutputFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m',{feature}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moutputFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'allSituationalChoicesWithFeatures' is not defined"
     ]
    }
   ],
   "source": [
    "# RECORD ALL FEATURES PRE AND VIA INCLUSIVE WITH PRE AND VIA COUNTED SEPARATELY\n",
    "inputFilePath = '/Volumes/Storage/Programming/dissertation-research/csv-data/dynamic-features-gospels.tsv'\n",
    "\n",
    "preTextFeatureHeaders = [f'pre-{feature}' for feature in allSituationalChoices if feature != 'no embedded discourse' and feature != 'excluded discourse only']\n",
    "\n",
    "viaTextFeatureHeaders = [f'via-{feature}' for feature in allSituationalChoices if feature != 'no embedded discourse' and feature != 'excluded discourse only']\n",
    "\n",
    "combinedFeatureHeaders = [*preTextFeatureHeaders, *viaTextFeatureHeaders]\n",
    "\n",
    "with open(inputFilePath, encoding='utf8') as inputFile:\n",
    "    with open('PRE-VIA-vectors-gospels.tsv', 'a', encoding='utf8') as outputFile:\n",
    "        outputFile.write(\"SITUATION\")\n",
    "        for feature in allSituationalChoicesWithFeatures:\n",
    "            outputFile.write(',{feature}'.format(feature=feature))\n",
    "        outputFile.write('\\n')\n",
    "\n",
    "        for situationRow in inputFile:\n",
    "            outputRow = [] \n",
    "            cells = situationRow.split('\\t')\n",
    "            id = cells[0]\n",
    "            title = ';'.join(cells[1].split(',')) # NOTE: semicolon separator because titles are initially CSV\n",
    "            outputRow.append(f'{id} {title}')\n",
    "            pre = cells[2]\n",
    "            via = cells[3]\n",
    "            try:\n",
    "                preList = eval(pre)\n",
    "                viaList = eval(via)\n",
    "                preTextFeatures_sorted = sorted(preList)\n",
    "                viaTextFeatures_sorted = sorted(viaList)\n",
    "                preTextFeatures_distinguised = [f'pre-{feature}' for feature in preTextFeatures_sorted]\n",
    "                viaTextFeatures_distinguised = [f'via-{feature}' for feature in viaTextFeatures_sorted]\n",
    "                for featureHeading in combinedFeatureHeaders:\n",
    "                    count = 0\n",
    "                    if featureHeading in preTextFeatures_distinguised:\n",
    "                        count += 1\n",
    "                    if featureHeading in viaTextFeatures_distinguised:\n",
    "                        count += 1\n",
    "                    outputRow.append(str(count))\n",
    "\n",
    "                outputFile.write(','.join(outputRow)) \n",
    "                outputFile.write('\\n')\n",
    "            except Exception as error:\n",
    "                # in case the feature list is only a string \n",
    "                # (e.g. 'no embedded discourse' or column header)\n",
    "                # print(error)\n",
    "                # print('Previous line error location: ' + line)\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "allSituationalChoicesWithFeatures = [feature for feature in allSituationalChoices if feature != 'no embedded discourse' and feature != 'excluded discourse only']\n",
    "# allSituationalChoicesWithFeatures\n",
    "allSituationalChoices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECORD ALL FEATURES PRE AND VIA INCLUSIVE with pre and via counted separately\n",
    "inputFilePath = '/Volumes/Storage/Programming/dissertation-research/csv-data/dynamic-features-gospels.tsv'\n",
    "\n",
    "allSituationalChoicesWithFeatures = [feature for feature in allSituationalChoices if feature != 'no embedded discourse' and feature != 'excluded discourse only']\n",
    "\n",
    "with open(inputFilePath, encoding='utf8') as inputFile:\n",
    "    with open('all-dynamic-features-vectors-gospels.tsv', 'a', encoding='utf8') as outputFile:\n",
    "        outputFile.write(\"SITUATION\")\n",
    "        for feature in allSituationalChoicesWithFeatures:\n",
    "            outputFile.write(',{feature}'.format(feature=feature))\n",
    "        outputFile.write('\\n')\n",
    "\n",
    "        for situationRow in inputFile:\n",
    "            outputRow = [] \n",
    "            cells = situationRow.split('\\t')\n",
    "            id = cells[0]\n",
    "            title = ';'.join(cells[1].split(',')) # NOTE: semicolon separator because titles are initially CSV\n",
    "            outputRow.append(f'{id} {title}')\n",
    "            pre = cells[2]\n",
    "            via = cells[3]\n",
    "            try:\n",
    "                preList = eval(pre)\n",
    "                viaList = eval(via)\n",
    "                preTextFeatures_sorted = sorted(preList)\n",
    "                viaTextFeatures_sorted = sorted(viaList)\n",
    "                for featureHeading in allSituationalChoicesWithFeatures:\n",
    "                    count = 0\n",
    "                    if featureHeading in preTextFeatures_sorted:\n",
    "                        count += 1\n",
    "                    if featureHeading in viaTextFeatures_sorted:\n",
    "                        count += 1\n",
    "                    outputRow.append(str(count))\n",
    "\n",
    "                outputFile.write(','.join(outputRow)) \n",
    "                outputFile.write('\\n')\n",
    "            except Exception as error:\n",
    "                # in case the feature list is only a string \n",
    "                # (e.g. 'no embedded discourse' or column header)\n",
    "                # print(error)\n",
    "                # print('Previous line error location: ' + line)\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from math import sqrt\n",
    "\n",
    "def getSumOfSquares(vectorWithoutId):\n",
    "    sumOfSquares = sum([value * value for value in vectorWithoutId])\n",
    "    return sumOfSquares\n",
    "\n",
    "def generateCosineSimilarities(vectors):\n",
    "    vectorsWithoutHeaders = vectors[1:]\n",
    "    comparisons = []\n",
    "    for vector in vectorsWithoutHeaders:\n",
    "        stageId = vector[0]\n",
    "        vectorWithoutStageId = [float(value) for value in vector[1:]]\n",
    "        sumOfSquares = getSumOfSquares(vectorWithoutStageId)\n",
    "        magnitude = sqrt(sumOfSquares)\n",
    "\n",
    "        cosineSimilarities = []\n",
    "        for secondVector in vectorsWithoutHeaders:\n",
    "            secondVectorWithoutId = [float(value) for value in secondVector[1:]]\n",
    "            secondSumOfSquares = getSumOfSquares(secondVectorWithoutId)\n",
    "            secondMagnitude = sqrt(secondSumOfSquares)\n",
    "            if sumOfSquares > 0 and secondSumOfSquares > 0:\n",
    "                dotProduct = dot(vectorWithoutStageId,secondVectorWithoutId,out=None)\n",
    "                crossMultipliedMagnitude = magnitude * secondMagnitude\n",
    "                cosineSimilarity = dotProduct / crossMultipliedMagnitude\n",
    "                if cosineSimilarity > 1:\n",
    "                    print('High cos value: ', cosineSimilarity, stageId)\n",
    "            else:\n",
    "                cosineSimilarity = 0\n",
    "            cosineSimilarities.append(cosineSimilarity)\n",
    "\n",
    "        similaritiesByStage = [stageId]\n",
    "        for similarity in cosineSimilarities:\n",
    "            similaritiesByStage.append(similarity)\n",
    "        comparisons.append(similaritiesByStage)\n",
    "    return comparisons\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "\n",
    "def generateCosineSimilaritiesUsingSKLearn(vectors):\n",
    "    vectorsWithoutHeaders = vectors[1:]\n",
    "    vectorIds = [vector[0] for vector in vectorsWithoutHeaders]\n",
    "    vectorsWithoutStageId = [vector[1:] for vector in vectorsWithoutHeaders]\n",
    "    numpy_string_vectors = np.array(vectorsWithoutStageId)\n",
    "    numpy_vectors = numpy_string_vectors.astype(np.float)\n",
    "    sparse_vectors = sparse.csr_matrix(numpy_vectors)\n",
    "    similarities = cosine_similarity(sparse_vectors)\n",
    "    similarities_trimmed = [value[0:2] for value in similarities]\n",
    "    # Add ids back into vector array\n",
    "    similaritiesWithIds = []\n",
    "    for count, vector in enumerate(similarities):\n",
    "        similaritiesWithIds.append([vectorIds[count], *[round(value,2) for value in vector]])\n",
    "    return similaritiesWithIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mutationsFeaturesFilePath = '/Volumes/Storage/Programming/dissertation-research/python/all-dynamic-features-vectors-gospels.tsv'\n",
    "\n",
    "with open(mutationsFeaturesFilePath, encoding='utf8') as mutationsFeaturesFile:\n",
    "    with open('all-dynamic-features-cosine-similarities-gospels.tsv', 'a', encoding='utf8') as outputFile:\n",
    "\n",
    "        isFirstRow = False # NOTE: refactor to remove first row check\n",
    "        columnHeaders = []\n",
    "        mutationVectors = []\n",
    "        for situationRow in mutationsFeaturesFile:\n",
    "            cells = situationRow.strip().split(',')\n",
    "            id = ';'.join(cells[0].split(',')) # NOTE: replace all commas in id with semicolon\n",
    "            columnHeaders.append(id)\n",
    "            mutationVectors.append(cells)\n",
    "            outputRow = [] \n",
    "            outputRow.append(id)\n",
    "\n",
    "        # comparisons = generateCosineSimilarities(mutationVectors)\n",
    "        comparisons_sklearn = generateCosineSimilaritiesUsingSKLearn(mutationVectors)\n",
    "        outputFile.write(','.join(columnHeaders))\n",
    "        outputFile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mutationsFeaturesFilePath = '/Volumes/Storage/Programming/dissertation-research/python/all-dynamic-features-vectors-gospels.tsv'\n",
    "with open(mutationsFeaturesFilePath, encoding='utf8') as mutationsFeaturesFile:\n",
    "\n",
    "        for situationRow in mutationsFeaturesFile:\n",
    "            if len(situationRow.split('\\n')) != 1:\n",
    "                print(situationRow.split('\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "comparisonStrings = [';'.join(vector[0].split(',')) + ',' + ','.join([str(value)[0:4] for value in vector[1:]]) for vector in comparisons_sklearn]\n",
    "# for i in comparisonStrings:\n",
    "#     print(len(i[0].split(';')))\n",
    "#     if len(i) != 1:\n",
    "#         print(i[1])\n",
    "for i in comparisonStrings:\n",
    "    values = i.split(',')\n",
    "    for count,j in enumerate(values[1:]):\n",
    "        if float(j) > 1:\n",
    "            print(values[0], j, 'location', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all-dynamic-features-cosine-similarities-gospels.tsv', 'a', encoding='utf8') as outputFile:\n",
    "    outputFile.write('\\n'.join(comparisonStrings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR DISTINGUISHED PRE AND VIA TEXT FEATURES\n",
    "mutationsFeaturesFilePath = '/Volumes/Storage/Programming/dissertation-research/python/PRE-VIA-vectors-gospels.tsv'\n",
    "\n",
    "with open(mutationsFeaturesFilePath, encoding='utf8') as mutationsFeaturesFile:\n",
    "    with open('PRE-VIA-cosine-similarities-gospels.tsv', 'a', encoding='utf8') as outputFile:\n",
    "\n",
    "        isFirstRow = False # NOTE: refactor to remove first row check\n",
    "        columnHeaders = []\n",
    "        mutationVectors = []\n",
    "        for situationRow in mutationsFeaturesFile:\n",
    "            cells = situationRow.strip().split(',')\n",
    "            id = ';'.join(cells[0].split(',')) # NOTE: replace all commas in id with semicolon\n",
    "            columnHeaders.append(id)\n",
    "            mutationVectors.append(cells)\n",
    "            outputRow = [] \n",
    "            outputRow.append(id)\n",
    "\n",
    "        # comparisons = generateCosineSimilarities(mutationVectors)\n",
    "        comparisons_sklearn = generateCosineSimilaritiesUsingSKLearn(mutationVectors)\n",
    "        outputFile.write(','.join(columnHeaders))\n",
    "        outputFile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisonStrings = [';'.join(vector[0].split(',')) + ',' + ','.join([str(value)[0:4] for value in vector[1:]]) for vector in comparisons_sklearn]\n",
    "# for i in comparisonStrings:\n",
    "#     print(len(i[0].split(';')))\n",
    "#     if len(i) != 1:\n",
    "#         print(i[1])\n",
    "for i in comparisonStrings:\n",
    "    values = i.split(',')\n",
    "    for count,j in enumerate(values[1:]):\n",
    "        if float(j) > 1:\n",
    "            print(values[0], j, 'location', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('PRE-VIA-cosine-similarities-gospels.tsv', 'a', encoding='utf8') as outputFile:\n",
    "    outputFile.write('\\n'.join(comparisonStrings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateEdgesFromCosineSimilarities(columnHeaders, cosineSimilaritiesRow):\n",
    "    # 'ID\\tSOURCE\\tTARGET\\tWEIGHT\\n'\n",
    "    cells = cosineSimilaritiesRow.split(',')\n",
    "    edges = []\n",
    "    columnHeadersWithoutIDColumn = columnHeaders[1:]\n",
    "    source = cells[0]\n",
    "    for count, cell in enumerate(cells):\n",
    "        if count == 0:\n",
    "            pass\n",
    "        else:\n",
    "            cell = cells[count]\n",
    "            if float(cell) > 0:\n",
    "                target = columnHeaders[count]\n",
    "                if target == source:\n",
    "                    pass\n",
    "                else:\n",
    "                    # edgesSets.append(targetAndSource)\n",
    "                    edge = (f'{source}-{target}', source, target, cell)\n",
    "                    edgeString = '\\t'.join(edge)\n",
    "                    if len(edgeString.split('\\t')) != 4:\n",
    "                        print('FAIL', edgeString.split('\\t'))\n",
    "                    edges.append(edgeString)\n",
    "    return edges\n",
    "\n",
    "def generateNodeFromCosineSimilarities(cosineSimilaritiesRow):\n",
    "    cells = cosineSimilaritiesRow.split(',')\n",
    "    id = cells[0]\n",
    "    label = cells[0]\n",
    "    averageSimilarity = sum([float(cell) for cell in cells[1:]]) / len(cells[1:])\n",
    "    \n",
    "    nodeFields = (id, label, averageSimilarity)\n",
    "    \n",
    "    node = '\\t'.join([str(field) for field in nodeFields])\n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cosineSimilaritiesInputFilePath = 'all-dynamic-features-cosine-similarities-gospels.tsv'\n",
    "cosineSimilaritiesInputFilePath = 'PRE-VIA-cosine-similarities-gospels.tsv'\n",
    "\n",
    "nodes = []\n",
    "edges = []\n",
    "edgesDict = dict()\n",
    "with open(cosineSimilaritiesInputFilePath, 'r', encoding='utf8') as cosineCSVData:\n",
    "    isFirstRow = True\n",
    "    for line in cosineCSVData:\n",
    "        if isFirstRow:\n",
    "            columnHeaders = line.split(',')\n",
    "            isFirstRow = False\n",
    "        else:\n",
    "            node = generateNodeFromCosineSimilarities(line)\n",
    "            nodes.append(node)\n",
    "            rowEdges = generateEdgesFromCosineSimilarities(columnHeaders, line)\n",
    "            for edge in rowEdges:\n",
    "                cells = edge.split('\\t')\n",
    "                targetAndSourceArray = [\n",
    "                    cells[2], \n",
    "                    cells[1]\n",
    "                ]\n",
    "                if targetAndSourceArray[0] != targetAndSourceArray[1]:\n",
    "                    targetAndSource = set(targetAndSourceArray)\n",
    "                    pairIndex = '-'.join(targetAndSource)\n",
    "                    edgesDict[pairIndex] = edge\n",
    "# prunedEdges = []\n",
    "# print('Pruning redundant edge data . . .')\n",
    "# totalEdges = len(edges)\n",
    "# for count,edge in enumerate(edges):\n",
    "#     print('Testing ', count, ' of ', totalEdges, flush=False, end='\\r')\n",
    "#     cells = edge.strip().split('\\t')\n",
    "#     targetAndSource = set([\n",
    "#         cells[2], \n",
    "#         cells[1]\n",
    "#     ])\n",
    "#     if targetAndSource not in edgesSets:\n",
    "#         prunedEdges.append(edge)\n",
    "#         edgesSets.append(targetAndSource)\n",
    "#     else:\n",
    "#         pass\n",
    "\n",
    "with open('nodes.tsv', 'a', encoding='utf8') as nodeFile:\n",
    "    nodeFile.write('ID\\tLABEL\\tAVERAGE_SIMILARITY\\n')\n",
    "    for node in nodes:\n",
    "        nodeFile.write(node)\n",
    "        nodeFile.write('\\n')\n",
    "with open('edges.tsv', 'a', encoding='utf8') as edgeFile:\n",
    "    edgeFile.write('ID\\tSOURCE\\tTARGET\\tWEIGHT\\n')\n",
    "    # for edge in prunedEdges:\n",
    "    for count,edgePair in enumerate(edgesDict):\n",
    "        edgeFile.write(edgesDict[edgePair])\n",
    "        edgeFile.write('\\n')\n",
    "\n",
    "# NOTE: for some reason this operatoin adds line breaks for some edge records (search resulting edges file for newlines followed immediately by tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'2', '3'} not in edge set, added\n{'2', '3'} in edge set\n"
     ]
    }
   ],
   "source": [
    "target = '3'\n",
    "source = '2'\n",
    "edgesSets = list()\n",
    "\n",
    "if target == source:\n",
    "    print(targetAndSource, 'match. passed.')\n",
    "    pass\n",
    "targetAndSource = set([target, source])\n",
    "if targetAndSource not in edgesSets:\n",
    "    print(targetAndSource, 'not in edge set, added')\n",
    "    edgesSets.append(targetAndSource)\n",
    "else:\n",
    "    print(targetAndSource, 'in edge set')\n",
    "\n",
    "target = '2'\n",
    "source = '3'\n",
    "\n",
    "if target == source:\n",
    "    print(targetAndSource, 'match. passed.')\n",
    "    pass\n",
    "targetAndSource = set([target, source])\n",
    "if targetAndSource not in edgesSets:\n",
    "    print(targetAndSource, 'not in edge set, added')\n",
    "    edgesSets.append(targetAndSource)\n",
    "else:\n",
    "    print(targetAndSource, 'in edge set')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[5, 5, 4, 4, 3, 3, 2, 2, 1, 1]"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "sorted([1,2,3,4,5,5,4,3,2,1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  3\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  7\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  5\n",
      "Edges saved:  1\n",
      "Edges saved:  7\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  5\n",
      "Edges saved:  1\n",
      "Edges saved:  4\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  4\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  4\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  4\n",
      "Edges saved:  3\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  3\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  4\n",
      "Edges saved:  4\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  4\n",
      "Edges saved:  3\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  4\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  4\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  3\n",
      "Edges saved:  1\n",
      "Edges saved:  6\n",
      "Edges saved:  1\n",
      "Edges saved:  4\n",
      "Edges saved:  3\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  4\n",
      "Edges saved:  5\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  4\n",
      "Edges saved:  1\n",
      "Edges saved:  3\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  3\n",
      "Edges saved:  3\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  5\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  3\n",
      "Edges saved:  1\n",
      "Edges saved:  4\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  4\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  3\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  3\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  3\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  3\n",
      "Edges saved:  3\n",
      "Edges saved:  7\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  2\n",
      "Edges saved:  3\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  2\n",
      "Edges saved:  2\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  3\n",
      "Edges saved:  2\n",
      "Edges saved:  4\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  3\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  3\n",
      "Edges saved:  2\n",
      "Edges saved:  3\n",
      "Edges saved:  4\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  5\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  3\n",
      "Edges saved:  4\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  4\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  2\n",
      "Edges saved:  4\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  4\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  1\n",
      "Edges saved:  2\n",
      "Edges saved:  1\n"
     ]
    }
   ],
   "source": [
    "# Query cosine similarities table and return an edge for each column or row's top value (for as many edges as attain that value)\n",
    "cosineSimilaritiesInputFilePath = 'PRE-VIA-cosine-similarities-gospels.tsv'\n",
    "\n",
    "def generateTopEdgesFromCosineSimilarities(columnHeaders, cosineSimilaritiesRow):\n",
    "    # 'ID\\tSOURCE\\tTARGET\\tWEIGHT\\n'\n",
    "    cells = cosineSimilaritiesRow.split(',')\n",
    "    edges = []\n",
    "    edgeTuples = []\n",
    "    columnHeadersWithoutIDColumn = columnHeaders[1:]\n",
    "    source = cells[0]\n",
    "    for count, cell in enumerate(cells):\n",
    "        if count == 0:\n",
    "            pass\n",
    "        else:\n",
    "            cell = cells[count]\n",
    "            if float(cell) > 0:\n",
    "                target = columnHeaders[count]\n",
    "                if target == source:\n",
    "                    pass\n",
    "                else:\n",
    "                    # edgesSets.append(targetAndSource)\n",
    "                    # edge = (f'{source}-{target}', source, target, cell)\n",
    "                    # edgeString = '\\t'.join(edge)\n",
    "                    # if len(edgeString.split('\\t')) != 4:\n",
    "                        # print('FAIL', edgeString.split('\\t'))\n",
    "                    edgeTuples.append((source,target,float(cell)))\n",
    "    edgeTuples.sort(key=lambda edgeTuple: edgeTuple[2])\n",
    "    topEdgeValue = edgeTuples[-1]\n",
    "    topEdges = [edge for edge in edgeTuples if edge[2] == topEdgeValue[2] ]\n",
    "    edges = ['\\t'.join((f'{edge[0]}-{edge[1]}', edge[0], edge[1], str(edge[2]))) for edge in topEdges]\n",
    "    print('Edges saved: ',len(edges))\n",
    "    return edges\n",
    "    \n",
    "\n",
    "nodes = []\n",
    "edges = []\n",
    "edgesDict = dict()\n",
    "with open(cosineSimilaritiesInputFilePath, 'r', encoding='utf8') as cosineCSVData:\n",
    "    isFirstRow = True\n",
    "    for line in cosineCSVData:\n",
    "        if isFirstRow:\n",
    "            columnHeaders = line.split(',')\n",
    "            isFirstRow = False\n",
    "        else:\n",
    "            node = generateNodeFromCosineSimilarities(line)\n",
    "            nodes.append(node)\n",
    "            rowEdges = generateTopEdgesFromCosineSimilarities(columnHeaders, line)\n",
    "            for edge in rowEdges:\n",
    "                cells = edge.split('\\t')\n",
    "                targetAndSourceArray = [\n",
    "                    cells[2], \n",
    "                    cells[1]\n",
    "                ]\n",
    "                if targetAndSourceArray[0] != targetAndSourceArray[1]:\n",
    "                    targetAndSource = set(targetAndSourceArray)\n",
    "                    pairIndex = '-'.join(targetAndSource)\n",
    "                    edgesDict[pairIndex] = edge\n",
    "# prunedEdges = []\n",
    "# print('Pruning redundant edge data . . .')\n",
    "# totalEdges = len(edges)\n",
    "# for count,edge in enumerate(edges):\n",
    "#     print('Testing ', count, ' of ', totalEdges, flush=False, end='\\r')\n",
    "#     cells = edge.strip().split('\\t')\n",
    "#     targetAndSource = set([\n",
    "#         cells[2], \n",
    "#         cells[1]\n",
    "#     ])\n",
    "#     if targetAndSource not in edgesSets:\n",
    "#         prunedEdges.append(edge)\n",
    "#         edgesSets.append(targetAndSource)\n",
    "#     else:\n",
    "#         pass\n",
    "\n",
    "with open('nodes_topEdgesOnly.tsv', 'a', encoding='utf8') as nodeFile:\n",
    "    nodeFile.write('ID\\tLABEL\\tAVERAGE_SIMILARITY\\n')\n",
    "    for node in nodes:\n",
    "        nodeFile.write(node)\n",
    "        nodeFile.write('\\n')\n",
    "with open('edges_topEdgesOnly.tsv', 'a', encoding='utf8') as edgeFile:\n",
    "    edgeFile.write('ID\\tSOURCE\\tTARGET\\tWEIGHT\\n')\n",
    "    # for edge in prunedEdges:\n",
    "    for count,edgePair in enumerate(edgesDict):\n",
    "        edgeFile.write(edgesDict[edgePair])\n",
    "        edgeFile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('source2', 'target2', 47), ('source1', 'target1', 99)]\n"
     ]
    }
   ],
   "source": [
    "testEdgeTuples = [('source1', 'target1', 99), ('source2', 'target2', 47)]\n",
    "testEdgeTuples.sort(key=lambda edgeTuple: edgeTuple[2])\n",
    "print(testEdgeTuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}